{"agent": "ppo", "states": {"type": "float", "shape": [4], "min_value": [-4.800000190734863, -3.0, -0.41887903213500977, -3.0], "max_value": [4.800000190734863, 3.0, 0.41887903213500977, 3.0]}, "actions": {"type": "int", "shape": [], "num_values": 2}, "max_episode_timesteps": 500, "batch_size": 12, "network": {"type": "auto", "rnn": false}, "use_beta_distribution": false, "memory": "minimum", "update_frequency": 1, "learning_rate": 0.001813150053725916, "subsampling_fraction": 0.9131375430837279, "optimization_steps": null, "likelihood_ratio_clipping": 0.09955676846552193, "discount": 0.9985351346308641, "predict_terminal_values": false, "baseline": {"type": "auto", "rnn": false}, "baseline_optimizer": {"optimizer": "adam", "learning_rate": 0.003670157218888348, "multi_step": 10}, "preprocessing": null, "exploration": 0.0, "variable_noise": 0.0, "l2_regularization": 0.0, "entropy_regularization": 0.0011393096635237982, "parallel_interactions": 1, "config": null, "saver": null, "summarizer": null, "recorder": {"directory": "test/data/ppo-traces", "start": 80}, "internals": {}, "initial_internals": {"policy": {}, "baseline": {}}}